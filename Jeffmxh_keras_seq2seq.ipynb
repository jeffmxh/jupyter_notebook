{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import #导入3.x的特征函数\n",
    "from __future__ import print_function\n",
    "from imp import reload\n",
    "import pandas as pd #导入Pandas\n",
    "import numpy as np #导入Numpy\n",
    "import jieba #导入结巴分词\n",
    "import gensim\n",
    "import h5py\n",
    "import sys\n",
    "import re\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "#from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.wrappers import Bidirectional #包装器，把一个层应用到输入的每一个时间步上\n",
    "from keras.callbacks import TensorBoard  #TensorBoard是TensorFlow提供的可视化工具，\n",
    "                                          #该回调函数将日志信息写入TensorBorad，\n",
    "                                          #使得你可以动态的观察训练和测试指标的图像以及不同层的激活值直方图。\n",
    "from keras.metrics import binary_accuracy #对二分类问题,计算在所有预测值上的平均正确率\n",
    "from keras import backend as K            #keras后端\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jieba.enable_parallel(32)\n",
    "cw = lambda x: list(jieba.cut(str(x))) #定义分词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_sub = lambda x:re.sub(\"\\W\", \"\", x)\n",
    "re_sub_vec = np.vectorize(re_sub)\n",
    "#re_sub(text.content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = list()\n",
    "for i in range(1,141):\n",
    "    with open('/home/yuangxue/abstract/news/training%d.txt' % (i)) as f:\n",
    "        a = f.readlines()\n",
    "        text.append(''.join(a))\n",
    "#     with open('/home/yuangxue/abstract/news/training%d.txt' % (i)) as f:\n",
    "#         a = f.readlines()\n",
    "#         text.append(''.join(a))\n",
    "text = pd.DataFrame({'content':text})\n",
    "text['content'] = re_sub_vec(text['content'])\n",
    "text['words'] = text['content'].apply(cw) #评论分词\n",
    "\n",
    "abstract = list()\n",
    "for i in range(1,141):\n",
    "    with open('/home/yuangxue/abstract/abstract/training%db.txt' % (i)) as f:\n",
    "        a = f.readlines()\n",
    "        abstract.append(''.join(a))\n",
    "#     with open('/home/yuangxue/abstract/abstract/training%da.txt' % (i)) as f:\n",
    "#         a = f.readlines()\n",
    "#         abstract.append(''.join(a))\n",
    "abstract = pd.DataFrame({'content':abstract})\n",
    "abstract['content'] = re_sub_vec(abstract['content'])\n",
    "abstract['words'] = abstract['content'].apply(cw) #评论分词\n",
    "\n",
    "text_len = 1200\n",
    "abstract_len = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordvec_model = gensim.models.word2vec.Word2Vec.load('/home/yuangxue/word2vec_from_weixin/word2vec/word2vec_wx')\n",
    "wordvec_model = gensim.models.word2vec.Word2Vec.load('/home/jeffmxh/word2vec/wiki.zh.text.model')\n",
    "wordvec_weight = wordvec_model.wv.syn0\n",
    "vocab = dict([(k, v.index) for k, v in wordvec_model.wv.vocab.items()])\n",
    "vocab_inv = {vocab[w]:w for w in vocab.keys()}\n",
    "word_embedding_layer = Embedding(\n",
    "        input_dim=wordvec_weight.shape[0],\n",
    "        output_dim=wordvec_weight.shape[1],\n",
    "        weights=[wordvec_weight],\n",
    "        trainable=False)\n",
    "word_to_id = lambda word: not (vocab.get(word) is None) and vocab.get(word) or 0 #词->index\n",
    "words_to_ids = lambda words: list(map(word_to_id, words))       \n",
    "\n",
    "# text['sent'] = list(sequence.pad_sequences(text['words'], maxlen=text_len, padding='post', truncating='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvec_model['开心'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text['sent'] = text['words'].apply(words_to_ids)                  #变成432100\n",
    "text['sent'] = list(sequence.pad_sequences(text['sent'], maxlen=text_len, padding='post', truncating='post'))\n",
    "abstract['sent'] = abstract['words'].apply(words_to_ids)\n",
    "abstract['sent'] = list(sequence.pad_sequences(abstract['sent'], maxlen=abstract_len, padding='post', truncating='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sent_list, length, dim = 256):\n",
    "    vec = np.zeros((length, dim), dtype = float)\n",
    "    for i, ch in enumerate(sent_list):\n",
    "        vec[i] = wordvec_model[vocab_inv[ch]]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xa = np.zeros((len(text), 1200, 300))\n",
    "ya = np.zeros((len(abstract), 20, 300))\n",
    "for i in range(len(text.sent)):\n",
    "    xa[i] = vectorize(text.sent[i], 1200, 300)\n",
    "for i in range(len(abstract.sent)): \n",
    "    ya[i] = vectorize(abstract.sent[i], 20, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = xa[::2] #训练集\n",
    "xt = xa[1::2] #测试集\n",
    "y = ya[::2] #训练集\n",
    "yt = ya[1::2] #测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(None,300), return_sequences=False))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(RepeatVector(20))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(300, activation=\"linear\")))\n",
    "model.compile(loss=\"mse\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_80 (LSTM)               (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "repeat_vector_39 (RepeatVect (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_81 (LSTM)               (None, 20, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 20, 300)           38700     \n",
      "=================================================================\n",
      "Total params: 406,444.0\n",
      "Trainable params: 406,444.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140 samples, validate on 70 samples\n",
      "Epoch 1/10\n",
      "140/140 [==============================] - 25s - loss: 0.7953 - val_loss: 0.7203\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 20s - loss: 0.7089 - val_loss: 0.6739\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 20s - loss: 0.6860 - val_loss: 0.6689\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 20s - loss: 0.6828 - val_loss: 0.6666\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 20s - loss: 0.6808 - val_loss: 0.6654\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 20s - loss: 0.6796 - val_loss: 0.6648\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 17s - loss: 0.6791 - val_loss: 0.6645\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 17s - loss: 0.6785 - val_loss: 0.6639\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 16s - loss: 0.6782 - val_loss: 0.6636\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 16s - loss: 0.6779 - val_loss: 0.6632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b5a5e7ac8>"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xa, ya, validation_data=(xt, yt), epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtest = np.zeros((1, 1200, 300))\n",
    "\n",
    "xtest[0] = x[0]\n",
    "\n",
    "b = model.predict(xtest)[0]\n",
    "# print(y[1])\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吾城吾乡原乡异域别人的城市在当下的现实语境中故乡是一个语义丰富而模糊的词语比如一年一度的春运大潮显然不仅仅是因为返乡的动因使然在全球化与城市化的进程中我们每一个人都不能不面对从故乡到异乡的流转与变迁即使从未离开过的家园也在高速发展的物质社会中不断蜕变吾城吾乡既是传统承袭的故园情怀也是重新来过的自我定位背后既有个人的生命体验亦是家国命运的折射我们选择了五座城乡八位摄影师以两两对照并持续数年拍摄的影像阐释这一命题摄影就像一只手悄悄地在现实和精神世界中放下地标在喧嚣与骚动中我们每一个人或许都是某种意义上的异乡人而摄影能否成为一个微弱而清晰的声音说看吾城吾乡专题按语李楠深圳南山区的南山村南园村北头村三村连成一片形成深圳最大的城中村这是透过飞机舷窗拍摄到的三村全貌2008年摄张新民图吾城吾乡原乡异域农村包围城市的大迁徙不仅仅是从原乡到异城的地理转移更是物质身份与精神身份的双重置换国家城市化进程中伴随的个人城市化注定是一场身心交战城市可以栖居却未必可以归属乡土无法归去却又藕断丝连所谓的寻根之旅往往是获得了表面零碎的记忆片断之后更为深刻的精神失落被父亲牵引着从城市重返原乡的孙京涛只能以外来者的目光重新打量他最熟悉的人们而远离故里的张新民跟随乡亲们一起见证了中国民工潮最终停靠在一座平空而起的城市深圳因为这样的经历他们平静的镜头里汹涌着跌宕起伏的宿命和无法停止的寻找原乡与异城或许将会是我们越来越难以抉择的矛盾在沙井镇川籍农民工聚居的棚户区春节期间很多人回不了故乡小杂货铺的电视机前经常挤满了老乡1997年摄张新民图别人的城市腊月二十三祭灶王爷老胡两口子百味杂陈汽车票买到了深圳到潼南680元一个人比平常贵一倍但明天终于可以回去过年了来深圳打工10年两口子只回去过两次在南方狭窄的出租屋里过年为的就是攒下钱来帮补儿女正儿八经在城里安家这个目标在2013年就要实现了儿子在重庆买了房春节结婚这样的喜事做父母的当然要提前几天回去但厂里硬是不批假放出话来要走可以自动离职这意味着老胡一个月的工钱是彻底拿不到了深圳的最低工资线是1500元老胡拿1800像他这样的川籍重庆籍农民工在深圳有很多他们靠出卖劳动力吃饭一干就是十几二十年从来不敢想象能住进这里的天价房他们住棚屋住城中村年复一年至今未变1997年我曾在沙井镇拍摄过我的老乡四川农民工聚居的棚户区后来棚户区没有了外来务工人员的落脚地是城中村或者城乡接合部的简陋出租屋二十多年过去深圳的城中村越来越密集功能越来越齐备小超市洗脚屋网吧饭馆菜市药店麻将桌老军医一应俱全我住的地方临街背后就是三个自然村连成一片的巨型城中村十几万租户将近一半操四川方言袭川渝风俗开个小店也忘不了腌一串腊肉种一棵盆栽养一只小狗沏一壶酽茶四川人的天性丝毫不变2013年摄张新民图胡榜武夫妇来自潼南县农村2003年来深圳打工至今很少回老家过年今年因为儿子春节结婚要回家了老胡把老家的照片取了下来给我看2013年摄张新民图二十多年过去这些特别能吃苦耐劳的人依然不是这座城市的主人他们看新闻联播里不断被刷新的GDP看路虎奔驰在马路上风驰电掣偶尔也去华强北看新起的地标地王去深圳湾看一水之隔的香港他们看着别人的城市日新月异我的父老乡亲们就这样一天天老去一些人慢慢地就淡忘了老家慢慢地就习惯了出租屋里的城市化前海某建筑工地川籍农民工背上的刺青人生最大痛苦失去自由1999年年摄张新民图'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('滨田省', 0.5977662205696106)\n",
      "('同年', 0.5889389514923096)\n",
      "('国姓', 0.5328131914138794)\n",
      "('末世', 0.5393956899642944)\n",
      "('同年', 0.5889389514923096)\n",
      "('同年', 0.5889389514923096)\n",
      "('小城镇', 0.4807368516921997)\n",
      "('同年', 0.5889389514923096)\n",
      "('同年', 0.5889389514923096)\n",
      "('同年', 0.5889389514923096)\n",
      "('主观', 0.5208644866943359)\n",
      "('约定俗成', 0.7271884679794312)\n",
      "('中曾', 0.5745320916175842)\n",
      "('思乡', 0.4522740840911865)\n",
      "('同年', 0.5889389514923096)\n",
      "('同年', 0.5889389514923096)\n",
      "('抽象概念', 0.7426023483276367)\n",
      "('同年', 0.5889389514923096)\n",
      "('同年', 0.5889389514923096)\n",
      "('主观', 0.5855385661125183)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(wordvec_model.similar_by_vector(x[0][i])[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('！', 1.0),\n",
       " ('吧', 0.7221462726593018),\n",
       " ('哦', 0.6962027549743652),\n",
       " ('啦', 0.6935625076293945),\n",
       " ('哟', 0.6884435415267944),\n",
       " ('噢', 0.6847633123397827),\n",
       " ('~', 0.6815340518951416),\n",
       " ('啊', 0.6630978584289551),\n",
       " ('!', 0.6576281189918518),\n",
       " ('咯', 0.652571439743042)]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_model.similar_by_vector(wordvec_model['！'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = np.array(list(text['sent']))[::2] #训练集\n",
    "xt1 = np.array(list(text['sent']))[1::2] #测试集\n",
    "xa1 = np.array(list(text['sent'])) #全集\n",
    "\n",
    "y1 = np.array(list(abstract['sent']))[::2] #训练集\n",
    "yt1 = np.array(list(abstract['sent']))[1::2] #测试集\n",
    "ya1 = np.array(list(abstract['sent'])) #全集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(word_embedding_layer)\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(RepeatVector(100))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(256, activation=\"linear\")))\n",
    "model.compile(loss=\"mse\", optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
